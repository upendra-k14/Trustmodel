impact of homophily term
Generating negative set...
Variables initialized
Loss: 11672.9990234375
Loss: 11503.6328125
Loss: 11450.455078125
Loss: 11386.880859375
Loss: 11408.80078125
Loss: 11072.931640625
Loss: 11271.2119140625
Loss: 11103.5654296875
Loss: 11117.9921875
Loss: 11163.552734375
Loss: 10754.5732421875
Loss: 10842.810546875
Loss: 10809.765625
Loss: 10585.419921875
Loss: 10629.677734375
Loss: 10527.962890625
Loss: 10557.482421875
Loss: 10408.0654296875
Loss: 10357.9765625
Loss: 10258.49609375
Loss: 10127.4853515625
Loss: 10136.205078125
Loss: 9997.55078125
Loss: 10050.9140625
Loss: 9909.8134765625
-----------------------------------------
Iteration 250
Test Accuracy: 0.2727960650775634
-----------------------------------------
-----------------------------------------
Iteration 250
Train Accuracy: 0.571164898056516
-----------------------------------------
Loss: 86917.8359375
Loss: 74259.671875
Loss: 64734.7890625
Loss: 56084.83984375
Loss: 49258.453125
Loss: 43006.7109375
Loss: 37808.109375
Loss: 32978.1015625
Loss: 29224.734375
Loss: 25657.75
Loss: 22591.828125
Loss: 20118.4921875
Loss: 18009.65234375
Loss: 16201.453125
Loss: 14641.8779296875
Loss: 13502.4638671875
Loss: 12483.1884765625
Loss: 11412.3505859375
Loss: 10832.791015625
Loss: 10208.390625
Loss: 9828.884765625
Loss: 9399.44140625
Loss: 9088.201171875
Loss: 8840.474609375
Loss: 8685.6533203125
-----------------------------------------
Iteration 500
Test Accuracy: 0.22337584295920412
-----------------------------------------
-----------------------------------------
Iteration 500
Train Accuracy: 0.11587456778347442
-----------------------------------------
Loss: 8313.689453125
Loss: 8181.24951171875
Loss: 8026.8896484375
Loss: 7734.0546875
Loss: 7639.17822265625
Loss: 7536.06396484375
Loss: 7364.15673828125
Loss: 7185.15673828125
Loss: 7067.38037109375
Loss: 7044.001953125
Loss: 6852.08056640625
Loss: 6722.53955078125
Loss: 6622.25927734375
Loss: 6456.33349609375
Loss: 6362.4287109375
Loss: 6255.6328125
Loss: 6233.7646484375
Loss: 6037.640625
Loss: 6047.912109375
Loss: 5870.00244140625
Loss: 5716.27001953125
Loss: 5746.4892578125
Loss: 5656.359375
Loss: 5559.3642578125
Loss: 5433.2236328125
-----------------------------------------
Iteration 750
Test Accuracy: 0.3230397720949901
-----------------------------------------
-----------------------------------------
Iteration 750
Train Accuracy: 0.6500775008942411
-----------------------------------------
Loss: 5394.25048828125
Loss: 5289.28076171875
Loss: 5206.4423828125
Loss: 5113.7978515625
Loss: 5108.35205078125
Loss: 5004.03564453125
Loss: 4922.22998046875
Loss: 4780.04736328125
Loss: 4720.578125
Loss: 4731.2890625
Loss: 4494.6767578125
Loss: 4601.4697265625
Loss: 4485.56005859375
Loss: 4373.76708984375
Loss: 4327.78759765625
Loss: 4288.046875
Loss: 4188.4580078125
Loss: 4140.529296875
Loss: 4144.34423828125
Loss: 4104.72607421875
Loss: 3961.431884765625
Loss: 3967.0302734375
Loss: 3825.183837890625
Loss: 3796.78466796875
Loss: 3749.8095703125
-----------------------------------------
Iteration 1000
Test Accuracy: 0.3227059268656384
-----------------------------------------
-----------------------------------------
Iteration 1000
Train Accuracy: 0.6995206867771552
-----------------------------------------
Loss: 3781.19970703125
Loss: 3582.42431640625
Loss: 3560.53125
Loss: 3583.781982421875
Loss: 3463.932861328125
Loss: 3451.3994140625
Loss: 3411.562744140625
Loss: 3436.87841796875
Loss: 3327.259765625
Loss: 3314.150634765625
Loss: 3268.2255859375
Loss: 3179.899658203125
Loss: 3116.976318359375
Loss: 3061.556884765625
Loss: 3013.517578125
Loss: 2977.663818359375
Loss: 2903.291015625
Loss: 2895.88134765625
Loss: 2901.90625
Loss: 2847.1806640625
Loss: 2807.579345703125
Loss: 2770.805419921875
Loss: 2737.54443359375
Loss: 2591.671142578125
Loss: 2615.0703125
-----------------------------------------
Iteration 1250
Test Accuracy: 0.3113329327190581
-----------------------------------------
-----------------------------------------
Iteration 1250
Train Accuracy: 0.7015428639561225
-----------------------------------------
Loss: 2619.972412109375
Loss: 2547.782958984375
Loss: 2479.412841796875
Loss: 2439.6259765625
Loss: 2402.842041015625
Loss: 2404.91357421875
Loss: 2412.9375
Loss: 2329.196044921875
Loss: 2308.46875
Loss: 2114.110595703125
Loss: 2263.96826171875
Loss: 2133.009521484375
Loss: 2131.8115234375
Loss: 1916.739013671875
Loss: 2007.794677734375
Loss: 1964.5107421875
Loss: 1631.312255859375
Loss: 1592.5947265625
Loss: 1781.3095703125
Loss: 1597.8443603515625
Loss: 1416.084228515625
Loss: 1556.7000732421875
Loss: 1494.832763671875
Loss: 1326.054931640625
Loss: 1102.009521484375
-----------------------------------------
Iteration 1500
Test Accuracy: 0.2872070508112439
-----------------------------------------
-----------------------------------------
Iteration 1500
Train Accuracy: 0.6721783712888995
-----------------------------------------
##########################################
Margin 1.0
a1 0.0, a2 0.0, a3 0.1 a4 0.1
Average accuracy 0.29007626508794965
Number of iterations 1501
##########################################
Generating negative set...
Variables initialized
Loss: 11696.958984375
Loss: 11603.345703125
Loss: 11577.072265625
Loss: 11532.1318359375
Loss: 11520.66015625
Loss: 11503.3203125
Loss: 11387.5947265625
Loss: 11223.8681640625
Loss: 11132.640625
Loss: 11128.84765625
Loss: 11246.6669921875
Loss: 10993.5390625
Loss: 10941.806640625
Loss: 10837.8154296875
Loss: 10844.8115234375
Loss: 10949.3642578125
Loss: 10639.416015625
Loss: 10764.7666015625
Loss: 10423.62890625
Loss: 10407.6015625
Loss: 10394.4423828125
Loss: 10438.849609375
Loss: 10375.701171875
Loss: 10254.0146484375
Loss: 10169.6435546875
-----------------------------------------
Iteration 250
Test Accuracy: 0.26626382675658233
-----------------------------------------
-----------------------------------------
Iteration 250
Train Accuracy: 0.5707356623345654
-----------------------------------------
Loss: 563494.1875
Loss: 462060.90625
Loss: 391354.96875
Loss: 328021.5625
Loss: 278192.15625
Loss: 236400.34375
Loss: 195837.953125
Loss: 162786.515625
Loss: 137133.234375
Loss: 113061.34375
Loss: 92901.984375
Loss: 74721.203125
Loss: 60079.953125
Loss: 49669.9453125
Loss: 39615.1484375
Loss: 31951.234375
Loss: 26021.04296875
Loss: 21441.193359375
Loss: 17424.19921875
Loss: 14932.265625
Loss: 13176.2880859375
Loss: 11745.951171875
Loss: 10791.5166015625
Loss: 10214.689453125
Loss: 9758.158203125
-----------------------------------------
Iteration 500
Test Accuracy: 0.19753622220738465
-----------------------------------------
-----------------------------------------
Iteration 500
Train Accuracy: 0.09728389173721236
-----------------------------------------
Loss: 9598.197265625
Loss: 9405.8994140625
Loss: 9137.62890625
Loss: 9003.2138671875
Loss: 8895.5751953125
Loss: 8588.271484375
Loss: 8475.1484375
Loss: 8258.8427734375
Loss: 8180.98095703125
Loss: 7926.34130859375
Loss: 7787.2734375
Loss: 7702.02294921875
Loss: 7571.33544921875
Loss: 7388.9189453125
Loss: 7254.48095703125
Loss: 7167.99365234375
Loss: 6872.77685546875
Loss: 6806.189453125
Loss: 6579.09521484375
Loss: 6386.5439453125
Loss: 6207.28955078125
Loss: 5871.037109375
Loss: 5651.8076171875
Loss: 5268.72021484375
Loss: 4753.8486328125
-----------------------------------------
Iteration 750
Test Accuracy: 0.3222941844161047
-----------------------------------------
-----------------------------------------
Iteration 750
Train Accuracy: 0.48251341361631095
-----------------------------------------
Loss: 3928.256591796875
Loss: 3301.9462890625
Loss: 2461.91796875
Loss: 1222.8349609375
Loss: 147.2041015625
Loss: -1463.3642578125
Loss: -3274.71435546875
Loss: -5744.04345703125
Loss: -9196.662109375
Loss: -12816.9580078125
Loss: -17718.87109375
Loss: -23037.8671875
Loss: -28813.8671875
Loss: -39349.5546875
Loss: -42741.6796875
Loss: -52626.390625
Loss: -64830.5625
Loss: -75339.7421875
Loss: -97986.7109375
Loss: -108644.921875
Loss: -132656.78125
Loss: -159131.515625
Loss: -199202.703125
Loss: -227065.0
Loss: -250740.15625
-----------------------------------------
Iteration 1000
Test Accuracy: 0.2985800449578242
-----------------------------------------
-----------------------------------------
Iteration 1000
Train Accuracy: 0.39072373912006675
-----------------------------------------
Loss: -296903.84375
Loss: -360404.84375
Loss: -409085.09375
Loss: -457679.625
Loss: -499134.5
Loss: -587238.25
Loss: -709128.1875
Loss: -778736.875
Loss: -923365.25
Loss: -1016033.875
Loss: -1157844.25
Loss: -1229576.125
Loss: -1481697.25
Loss: -1568588.375
Loss: -1755221.375
Loss: -2143833.5
Loss: -2430831.0
Loss: -2613092.0
Loss: -2838123.0
Loss: -3272249.25
Loss: -3494300.75
Loss: -3843336.75
Loss: -4215197.5
Loss: -4933033.5
Loss: -5339866.5
-----------------------------------------
Iteration 1250
Test Accuracy: 0.27770358994903294
-----------------------------------------
-----------------------------------------
Iteration 1250
Train Accuracy: 0.357887206390843
-----------------------------------------
Loss: -6178775.0
Loss: -6860884.0
Loss: -7885526.5
Loss: -7730467.0
Loss: -9153738.0
Loss: -9934726.0
Loss: -10920920.0
Loss: -11362913.0
Loss: -12323020.0
Loss: -13328881.0
Loss: -14408282.0
Loss: -16352787.0
Loss: -17320834.0
Loss: -19889970.0
Loss: -21123924.0
Loss: -23427472.0
Loss: -25578432.0
Loss: -30829824.0
Loss: -28561530.0
Loss: -28618250.0
Loss: -31533286.0
Loss: -33823604.0
Loss: -41607040.0
Loss: -40127796.0
Loss: -47424024.0
-----------------------------------------
Iteration 1500
Test Accuracy: 0.2613563018851127
-----------------------------------------
-----------------------------------------
Iteration 1500
Train Accuracy: 0.3460403004650054
-----------------------------------------
